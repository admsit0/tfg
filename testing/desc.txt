Aqu√≠ tienes el *resumen completo y estructurado del TFG* basado √∫nicamente en la informaci√≥n discutida hasta ahora en este chat.
Est√° organizado para servir directamente como *contexto exportable* a otro asistente o sistema que deba continuar el trabajo.

---

## üß© *Resumen integral del TFG: Geometr√≠a de activaciones, regularizaci√≥n y explicabilidad*

### *1. Prop√≥sito del trabajo*

El Trabajo de Fin de Grado (TFG) busca entender *c√≥mo la regularizaci√≥n afecta la geometr√≠a interna de las activaciones en redes neuronales* y c√≥mo esta relaci√≥n influye en la capacidad del modelo para generalizar y evitar el sobreajuste (overfitting).

Los *objetivos principales* son:

* Analizar la *estructura geom√©trica* del espacio de activaciones (representaciones internas) generado por la red.
* Evaluar la influencia de distintos tipos y grados de regularizaci√≥n (L1, L2, Dropout) sobre esa geometr√≠a.
* Cuantificar la *complejidad interna del modelo* a trav√©s del n√∫mero de ‚Äúestados ocultos √∫nicos‚Äù (unique hidden states).
* Explorar si existe un equilibrio (sweet spot) entre el n√∫mero de estados y la capacidad de generalizaci√≥n, de forma que el modelo no memorice ni colapse su representaci√≥n.

Los *objetivos secundarios* incluyen:

* Relacionar las m√©tricas geom√©tricas con el rendimiento (accuracy, p√©rdida) durante el entrenamiento.
* Sentar bases para la *explicabilidad* de los errores mediante an√°lisis del espacio de activaciones y su relaci√≥n con las etiquetas verdaderas y predichas.

---

### *2. Estado actual del proyecto*

El proyecto se encuentra en una fase avanzada de experimentaci√≥n y an√°lisis inicial de resultados.
Se ha completado una *serie de entrenamientos exhaustiva* con m√∫ltiples configuraciones de regularizaci√≥n:

* M√©todos de regularizaci√≥n probados:

  * *Baseline (sin regularizaci√≥n)*
  * *Dropout*, con probabilidades p ‚àà {0.0, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5}
  * *L1 regularization*, con pesos Œª ‚àà {0.0, 1.0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6}
  * *L2 regularization*, con pesos Œª ‚àà {0.0, 1.0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6}

Cada experimento se ha ejecutado con *5 folds* de validaci√≥n cruzada.

Por cada fold se guardan:

* M√©tricas de entrenamiento (metrics.csv) con columnas:

  * epoch, time, train_acc, train_loss, val_acc, val_loss
* Activaciones (activations_collected.csv):

  * Dimensiones: num_images √ó num_neurons (8000 √ó 64)
  * Cada fila corresponde a una imagen y cada columna a la activaci√≥n de una neurona.
  * En algunos casos se planea incluir tambi√©n pred_label y true_label.

#### *Resultados actuales*

* Se ha calculado el n√∫mero medio y desviaci√≥n est√°ndar de *estados √∫nicos* (num_states_mean, num_states_std) y su relaci√≥n con la *precisi√≥n de validaci√≥n* (val_acc_mean, val_acc_std).
* Resultados globales:

  * Regularizaciones leves (Dropout ‚âà 0.1‚Äì0.25, L1 ‚âà 1e-5‚Äì1e-4, L2 ‚âà 1e-5‚Äì1e-3) mantienen buena precisi√≥n (‚âà0.84‚Äì0.86) con una complejidad razonable.
  * Regularizaciones extremas (L1=1.0, L1=1e-1, L2=1.0) producen *colapso de activaciones* o saturaci√≥n: p√©rdida de precisi√≥n (<0.33) y n√∫mero de estados an√≥malo (p.ej. L1=1e-1 con ‚âà12 000 estados, std=0).
  * El baseline (sin regularizaci√≥n) tiene ‚âà11 500 estados y precisi√≥n ‚âà0.85, pero muestra *muchas neuronas inactivas*.
* Se ha detectado que algunos casos con p=0.0 o Œª=0.0 no coinciden exactamente con el baseline, lo que sugiere peque√±as inconsistencias en las semillas o folds.

---

### *3. Sistema y herramientas*

* *Lenguaje de programaci√≥n:* Python
* *Librer√≠as:*

  * PyTorch (entrenamiento y definici√≥n del modelo)
  * Pandas (manejo de CSVs)
  * NumPy (operaciones vectoriales y an√°lisis num√©rico)
  * Matplotlib (visualizaci√≥n)
  * Scikit-learn (c√°lculo de distancias, clustering)
* *Entorno:* entorno local de ejecuci√≥n controlado, con experimento completo ejecutado en ‚âà5 horas.
* *Dataset:* conjunto similar a Fashion-MNIST con 8000 im√°genes de entrenamiento y 64 neuronas ocultas.
* *Estructura de archivos:*

  
  runs/
    fashion_cv_dropout_l1/
      dropout_p_0.1/
        fold1/
          metrics.csv
          activations_collected.csv
        ...
      l1_weight_1e-4/
        fold1/
        ...
      baseline/
  
* *Criterio de igualdad entre activaciones:*
  Dos vectores se consideran ‚Äúiguales‚Äù si su distancia eucl√≠dea < Œµ (definido emp√≠ricamente).
  Se utilizan histogramas de distancias para elegir Œµ apropiado.

---

### *4. Problemas actuales*

1. *Regularizaci√≥n extrema (L1=1.0 y L1=1e-1):*

   * Genera activaciones colapsadas o saturadas (posiblemente todos los pesos ‚âà0).
   * Precisi√≥n muy baja y nula desviaci√≥n est√°ndar en num_states ‚Üí posible degeneraci√≥n num√©rica.

2. *Inconsistencias entre baseline y reg=0.0:*

   * M√©tricas ligeramente distintas sugieren que las *semillas aleatorias* o la *asignaci√≥n de folds* no son completamente reproducibles.

3. **Falta de etiquetas guardadas (pred_label, true_label):**

   * Limita la posibilidad de hacer an√°lisis de explicabilidad o de pureza de clusters.

4. *Selecci√≥n del umbral Œµ:*

   * La m√©trica de estados √∫nicos depende fuertemente del umbral de cercan√≠a.
   * A√∫n no se ha implementado un m√©todo autom√°tico para elegirlo basado en la distribuci√≥n de distancias.

5. *Representaci√≥n inactiva:*

   * En el baseline, varias neuronas no se activan (activaci√≥n = 0), lo que puede indicar saturaci√≥n o arquitectura sobredimensionada.

---

### *5. Contexto relevante adicional*

* El *an√°lisis de activaciones* busca capturar la ‚Äúgeometr√≠a interna‚Äù del conocimiento del modelo.
* Se observa que la regularizaci√≥n:

  * *Reduce el n√∫mero de estados* cuando es moderada (mejor generalizaci√≥n).
  * *Destruye la representaci√≥n* cuando es excesiva (colapso).
* Se sospecha que el n√∫mero de estados deber√≠a ser del orden de (num_points / num_classes) para lograr equilibrio entre variabilidad y generalizaci√≥n.
* Se ha definido la m√©trica clave:

  * num_states = n√∫mero de clusters en el espacio de activaciones seg√∫n umbral Œµ.
* Se ha planeado relacionar esta m√©trica con:

  * Precisi√≥n de entrenamiento y validaci√≥n.
  * Tipo e intensidad de regularizaci√≥n.
  * Tiempo de convergencia.
* Gr√°ficas principales propuestas:

  * num_hidden_states (X) vs accuracy (Y)
    (l√≠neas separadas para entrenamiento y validaci√≥n)
  * Curvas por tipo de regularizador y par√°metro.

---

### *6. Lo que falta o est√° pendiente*

1. *Guardado de etiquetas:*

   * Implementar guardado de pred_label y true_label junto a las activaciones o en archivo separado.
   * Permitir an√°lisis de:

     * Pureza de clusters (coherencia por clase)
     * Entrop√≠a de activaciones por clase
     * An√°lisis de confusiones geom√©tricas

2. *Nuevas m√©tricas geom√©tricas:*

   * Entrop√≠a de activaciones por clase.
   * Distancias intra/inter clase.
   * Medidas de separabilidad.
   * Correlaci√≥n entre pureza y rendimiento.

3. *Tratamiento de folds:*

   * Definir si se promedian resultados o se analizan por separado.
   * Implementar agregaci√≥n controlada para m√©tricas globales.

4. *Documentaci√≥n:*

   * Crear README.md avanzado con descripci√≥n de la arquitectura, par√°metros de configuraci√≥n (config.yaml), estructura de archivos y flujo de trabajo completo.

5. *An√°lisis avanzado:*

   * Revisi√≥n de los casos an√≥malos (L1 y L2 extremos).
   * An√°lisis visual del espacio de activaciones (p. ej. clustering DBSCAN o histogramas de distancias).
   * Estudio de correlaciones entre m√©tricas geom√©tricas y rendimiento.

6. *Optimizaci√≥n del pipeline:*

   * Automatizar el proceso de carga, c√°lculo y visualizaci√≥n de m√©tricas en un notebook √∫nico.

---

### *7. S√≠ntesis final*

El TFG est√° orientado a *cuantificar y explicar c√≥mo las t√©cnicas de regularizaci√≥n modifican la estructura interna del conocimiento de una red neuronal*, utilizando la geometr√≠a de las activaciones como ventana explicativa.
El trabajo cuenta ya con un conjunto s√≥lido de experimentos y resultados cuantitativos, pero requiere consolidar la parte de *explicabilidad* y *an√°lisis cualitativo* de los estados internos para cerrar el ciclo de interpretaci√≥n.

---